

020-01-30 Activities and Actions (not yet committed. Will commit once the to dos are completed, this evening likely)
 
 
1. Created new tasks, taskRunner, and taskDefintions allowing for reduce code and easy expansion to new tasks without forcing tasks to fit any require except that they need to be a class and one method should be callable to execute. Added a task result which no longer returns file details (wasn't used).
2. Refactored cli-predictor.py (for cron jobs, etc) to load taskdefinitions, and use task runner to execute (on background threads)
3. Refactored just_run_it.py (for interactive running / debugging) to use task definitions (dynamically). add -d option to display task definition without running. Modified to list tasks from task definitions.
4. Tested
5. Added new status check which just emits file list and details and list any running tasks
6. Updated yaml script to auto install crowdstrike on build
7. Updated yaml install and configure auto update for apt and reboot when needed (work in progress)
8. Fixed a bug that had required datashim files to appear in 2 locations.
9. Refactored all tasks to be self sufficient (they load their own data path, etc) to avoid needing to pass args to primary method.
10. Tested more
 
To Do: 
1. Refactor application.py and html front to move from mlRunner to taskRunner (including new results) for getting current status as well as starting tasks. Possibly display full task details.
2. Refactor application.py and html to break up files by task, potentially displaying dependency and output source E.G. : [file-name], size, date, is current, generated by taskname
3. Clean / purge all deprecated code including mlRunner, etc.
 
a. Possibly extend to add a queue for task requests (with queue management including cancellation, etc).
b. Possibly update to use standard python logger instead of cheesy hand brewed including logging detail levels.
c. Means to track and record last task or task run history outside of the log files for quick viewing.
d. Create a 2nd YAML that creates the c4.8xlarge and just runs the cli-predictor.py so we can use the web front end on a low cost unit and then the big host only runs for 1 hour every 2 weeks.
e. Possibly modify mapper to use a model whereby you can define new mapping tasks with the ability to identify or provide a means to map between two data sources and emit a third with some or all of the source columns. (Note, this likely needs more of a data shims model as data sources like the palo ato require code to map IP to IP block to find in PA.


Open Ideas and Enhancements:


2. High cost - introduce web front end with dynamic filter creation, output field selection against csv
3. Low cost - introduce new .env config for API host. Modify web server to use calls to that remote host for ML processing.  Shut down ML host when not in use.  
4. Med Cost -  Revist feature engineering for Vuln Catagory feature engineering to see if we can use McCune method for dropping processing time from 15 minutes down.
5. Low Cost - Refactor mlRunner to use a factory pattern for the boilerplate control code, passing in dependency files, output files, and method to invoke, modify result pieces to send back message based upon call result state
7. Low Cost - Figure out why splitter doesn't reject request to rerun when files are current
8. Low Cost - Get DNS entry and appropriate NIKE issued cert for https
9. Low Cost - Turn on some authentication model (cert easiest, okta hardest)
